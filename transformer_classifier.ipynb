{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-23 10:27:29.664568: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-23 10:27:29.664644: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = 'aclImdb_v1/train'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading files and labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_arr, data_labels = [], []\n",
    "\n",
    "for x in os.listdir(path_train):\n",
    "    if not x == 'unsup':\n",
    "        if x == 'pos':\n",
    "            pos_path = os.path.join(path_train, x)\n",
    "            for pos in os.listdir(pos_path):\n",
    "                file_pos_path = os.path.join(pos_path, pos)\n",
    "                with open(file_pos_path, 'r') as p:\n",
    "                    data = p.readlines()\n",
    "                    lines = []\n",
    "                    for line in data:\n",
    "                        s = line.replace('<br />', '')\n",
    "                        s = re.sub('[.][.][.]*', '', s)\n",
    "                        s = re.sub(\"([.,?!()])\", r' \\1', s)\n",
    "                        s = s.split()\n",
    "                        s = s[:60]\n",
    "                        s = \" \".join(s)\n",
    "                        lines.append(s)\n",
    "                        data_arr.append(lines)\n",
    "                        lines= []\n",
    "                        label = 1\n",
    "                        data_labels.append(label)\n",
    "\n",
    "        if x == 'neg':\n",
    "            neg_path = os.path.join(path_train, x)\n",
    "            for neg in os.listdir(neg_path):\n",
    "                file_neg_path = os.path.join(neg_path, neg)\n",
    "                with open(file_neg_path, 'r') as p:\n",
    "                    data = p.readlines()\n",
    "                    lines = []\n",
    "                    for line in data:\n",
    "                        s = line.replace('<br />', '')\n",
    "                        s = re.sub('[.][.][.]*', '', s)\n",
    "                        s = re.sub(\"([.,?!()])\", r' \\1', s)\n",
    "                        s = s.split()\n",
    "                        s = s[:60]\n",
    "                        s = \" \".join(s)\n",
    "                        lines.append(s)\n",
    "                        data_arr.append(lines)\n",
    "                        lines = []\n",
    "                        label = 0\n",
    "                        data_labels.append(label)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"strings_tokenizer\"\n",
    "reload_bert = tf.saved_model.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = reload_bert.tokenize(data_arr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "for x in data_arr:\n",
    "    tokens = reload_bert.tokenize(x)\n",
    "    token = tokens.numpy()\n",
    "    X_train.append(token[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset(data, labels):\n",
    "    # data = tf.constant(data)\n",
    "    ds = tf.data.Dataset.from_tensors((data, labels))\n",
    "    ds = ds.batch(32)\n",
    "    return ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_padded = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 176)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = to_dataset(X_train_padded, data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = x.shape[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = reload_bert.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen = X_train_padded.shape[0]\n",
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlens = X_train_padded.shape[1]\n",
    "maxlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 32\n",
    "num_heads = 2\n",
    "ff_dim = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.layers.Input(shape=(maxlens,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlens, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer = TransformerBlock(embed_dim, num_heads,ff_dim)\n",
    "x = transformer(x)\n",
    "x = keras.layers.GlobalAveragePooling1D()(x)\n",
    "x = keras.layers.Dropout(0.1)(x)\n",
    "x = keras.layers.Dense(16, activation='relu')(x)\n",
    "x = keras.layers.Dropout(0.1)(x)\n",
    "outputs = keras.layers.Dense(2, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4/4 [==============================] - 3s 106ms/step - loss: 0.7178 - accuracy: 0.4510\n",
      "Epoch 2/20\n",
      "4/4 [==============================] - 0s 103ms/step - loss: 0.6960 - accuracy: 0.5000\n",
      "Epoch 3/20\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 0.6902 - accuracy: 0.5000\n",
      "Epoch 4/20\n",
      "4/4 [==============================] - 0s 105ms/step - loss: 0.6928 - accuracy: 0.5098\n",
      "Epoch 5/20\n",
      "4/4 [==============================] - 0s 98ms/step - loss: 0.6912 - accuracy: 0.5392\n",
      "Epoch 6/20\n",
      "4/4 [==============================] - 0s 98ms/step - loss: 0.6883 - accuracy: 0.5098\n",
      "Epoch 7/20\n",
      "4/4 [==============================] - 0s 100ms/step - loss: 0.6814 - accuracy: 0.6471\n",
      "Epoch 8/20\n",
      "4/4 [==============================] - 0s 98ms/step - loss: 0.6829 - accuracy: 0.6078\n",
      "Epoch 9/20\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 0.6832 - accuracy: 0.5784\n",
      "Epoch 10/20\n",
      "4/4 [==============================] - 0s 99ms/step - loss: 0.6775 - accuracy: 0.5882\n",
      "Epoch 11/20\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 0.6719 - accuracy: 0.5784\n",
      "Epoch 12/20\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 0.6657 - accuracy: 0.5980\n",
      "Epoch 13/20\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 0.6672 - accuracy: 0.6569\n",
      "Epoch 14/20\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 0.6632 - accuracy: 0.7059\n",
      "Epoch 15/20\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 0.6523 - accuracy: 0.7157\n",
      "Epoch 16/20\n",
      "4/4 [==============================] - 0s 89ms/step - loss: 0.6570 - accuracy: 0.6471\n",
      "Epoch 17/20\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 0.6389 - accuracy: 0.7647\n",
      "Epoch 18/20\n",
      "4/4 [==============================] - 1s 202ms/step - loss: 0.6365 - accuracy: 0.7353\n",
      "Epoch 19/20\n",
      "4/4 [==============================] - 1s 121ms/step - loss: 0.6343 - accuracy: 0.7059\n",
      "Epoch 20/20\n",
      "4/4 [==============================] - 0s 101ms/step - loss: 0.6054 - accuracy: 0.7451\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(np.asarray(X_train_padded), np.asarray(data_labels), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2, 105,  87, ...,   0,   0,   0],\n",
       "       [  2,  50, 128, ...,   0,   0,   0],\n",
       "       [  2,  39, 153, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [  2,  83, 207, ...,   0,   0,   0],\n",
       "       [  2,  66,  73, ...,   0,   0,   0],\n",
       "       [  2,  39,   9, ...,   0,   0,   0]], dtype=int32)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = np.asarray(X_train_padded)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "predi = model.evaluate(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "predii = model.predict(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = []\n",
    "for x in predii:\n",
    "    predictions.append(tf.argmax(x).numpy())\n",
    "\n",
    "predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7cc056a87779a9cf21be25d923e268f9d95563a30d10c8f25f6225f96f103ebe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
